{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# As we use our own external modules, we need the folder src to be in the PYTHONPATH env variable.\n",
    "# However we do not expect the reader to add that folder to the env variable,\n",
    "# therefore we manually load it temporarily in each notebook.\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from modules.config import PATH_SCENARIOS, N_REDUCED_SCNEARIOS, N_SCENARIOS, PATH_SCENARIOS_REDUCED, PATH_SCENARIO_PROBABILITY"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scenario Reduction\n",
    "To reduce our scenarios we will use k-medoids clustering. k-medoids is very similar to k-means, however a cluster center in k-medoids is not the mean of all points belonging to that cluster, but rather a point of the cluster itself. For our purpose this is a lot more meaningful, as there can be no unrealistic centers, that have demand values which are not whole numbers.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scenarios = pd.read_pickle(PATH_SCENARIOS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We transform the dataframe so that one entry corresponds to exactly one scenario."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scenarios = scenarios.unstack(level=['start_hex_ids', 'end_hex_ids', 'time', 'vehicle_types'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kmedoids = KMedoids(\n",
    "    n_clusters=N_REDUCED_SCNEARIOS, \n",
    "    random_state=0, \n",
    "    ).fit(scenarios.values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We save which scenario was assigned to which center, so that we can later calculate the probability of each center."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scenario_reduction_assignment = pd.DataFrame(index = scenarios.index)\n",
    "scenario_reduction_assignment['cluster_label'] = kmedoids.labels_\n",
    "label_to_scenario_id_map = {i:indice for i,indice in enumerate(list(kmedoids.medoid_indices_))}\n",
    "scenario_reduction_assignment['cluster_label'] = scenario_reduction_assignment['cluster_label'].replace(label_to_scenario_id_map)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# note that because the scenarios dataframe was sorted the indices of the medoids are also the \n",
    "# scenario ids\n",
    "selected_scenario_ids = kmedoids.medoid_indices_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "selected_scenarios = scenarios.loc[selected_scenario_ids]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "selected_scenarios = selected_scenarios.stack(['start_hex_ids', 'end_hex_ids', 'time', 'vehicle_types'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scenario_id_list = list(selected_scenarios.index.get_level_values('scenarios').unique())\n",
    "scenario_reset_map = {id:i for i, id in enumerate(scenario_id_list)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "selected_scenarios = selected_scenarios.rename(index=scenario_reset_map)\n",
    "scenario_reduction_assignment['cluster_label'] = scenario_reduction_assignment['cluster_label'].replace(scenario_reset_map)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We calculate the probability of each center as the sum of the probabilities of the scenarios that are assigned to that center.   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scenario_probability =  scenario_reduction_assignment.reset_index().groupby('cluster_label').count()\n",
    "scenario_probability = scenario_probability.rename(columns={'scenarios': 'n_scenarios'})\n",
    "scenario_probability.index.names = ['scenarios']\n",
    "scenario_probability['probability'] = scenario_probability['n_scenarios'] / N_SCENARIOS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have the reduced number of scenarios. The last preperation we have to make is to reindex the scenarios so that the demands index contains the complete cartasian product of the regions. This is necessary as it might happen that there are no regions starting in a region, but there are trips ending there. As our model will only use one set of regions we need to address this issue."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hex_ids = {*selected_scenarios.index.get_level_values('start_hex_ids').unique()}\n",
    "hex_ids = list(hex_ids.union({*selected_scenarios.index.get_level_values('end_hex_ids').unique()}))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "complete_index = pd.MultiIndex.from_product(\n",
    "    [\n",
    "        selected_scenarios.index.get_level_values('scenarios').unique(),\n",
    "        pd.Index(hex_ids, name=\"start_hex_ids\"),\n",
    "        pd.Index(hex_ids, name=\"end_hex_ids\"),\n",
    "        selected_scenarios.index.get_level_values('time').unique(),\n",
    "        selected_scenarios.index.get_level_values('vehicle_types').unique(),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "selected_scenarios = selected_scenarios.reindex(complete_index, fill_value=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.makedirs(os.path.dirname(PATH_SCENARIO_PROBABILITY), exist_ok=True)\n",
    "scenario_probability.to_pickle(PATH_SCENARIO_PROBABILITY)\n",
    "\n",
    "os.makedirs(os.path.dirname(PATH_SCENARIOS_REDUCED), exist_ok=True)\n",
    "selected_scenarios.to_pickle(PATH_SCENARIOS_REDUCED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "selected_scenarios.reset_index().nunique()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "scenarios         4\n",
       "start_hex_ids    29\n",
       "end_hex_ids      29\n",
       "time              3\n",
       "vehicle_types     3\n",
       "demand           85\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "921dce5431bc8dd5032dbf768a24c906ac17844f87e0f443ecd3f0f6d3f457c9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('VR': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "98dda969eced25373352d8358f4a74bbe75f82753a7539c7817bd98c04161209"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}