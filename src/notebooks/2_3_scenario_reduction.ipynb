{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we use our own external modules, we need the folder src to be in the PYTHONPATH env variable.\n",
    "# However we do not expect the reader to add that folder to the env variable,\n",
    "# therefore we manually load it temporarily in each notebook.\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from modules.config import PATH_SCENARIOS, N_REDUCED_SCNEARIOS, N_SCENARIOS, PATH_SCENARIOS_REDUCED, PATH_SCENARIO_PROBABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario Reduction\n",
    "To reduce our scenarios we will use k-medoids clustering. k-medoids is very similar to k-means, however a cluster center in k-medoids is not the mean of all points belonging to that cluster, but rather a point of the cluster itself. For our purpose this is a lot more meaningful, as there can be no unrealistic centers, that have demand values which are not whole numbers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = pd.read_pickle(PATH_SCENARIOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "start_hex_ids     80\n",
       "end_hex_ids       80\n",
       "time               2\n",
       "vehicle_types      3\n",
       "scenarios          8\n",
       "demand           130\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "scenarios.reset_index().nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the dataframe so that one entry corresponds to exactly one scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = scenarios.unstack(level=['start_hex_ids', 'end_hex_ids', 'time', 'vehicle_types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmedoids = KMedoids(\n",
    "#     n_clusters=N_REDUCED_SCNEARIOS, \n",
    "#     random_state=0, \n",
    "#     method='pam', \n",
    "#     init=\"k-medoids++\"\n",
    "#     ).fit(X)\n",
    "kmedoids = KMedoids(\n",
    "    n_clusters=N_REDUCED_SCNEARIOS, \n",
    "    random_state=0, \n",
    "    ).fit(scenarios.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save which scenario was assigned to which center, so that we can later calculate the probability of each center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_reduction_assignment = pd.DataFrame(index = scenarios.index)\n",
    "scenario_reduction_assignment['cluster_label'] = kmedoids.labels_\n",
    "label_to_scenario_id_map = {i:indice for i,indice in enumerate(list(kmedoids.medoid_indices_))}\n",
    "scenario_reduction_assignment['cluster_label'] = scenario_reduction_assignment['cluster_label'].replace(label_to_scenario_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that because the scenarios dataframe was sorted the indices of the medoids are also the \n",
    "# scenario ids\n",
    "selected_scenario_ids = kmedoids.medoid_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_scenarios = scenarios.loc[selected_scenario_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_scenarios = selected_scenarios.stack(['start_hex_ids', 'end_hex_ids', 'time', 'vehicle_types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_id_list = list(selected_scenarios.index.get_level_values('scenarios').unique())\n",
    "scenario_reset_map = {id:i for i, id in enumerate(scenario_id_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_scenarios = selected_scenarios.rename(index=scenario_reset_map)\n",
    "scenario_reduction_assignment['cluster_label'] = scenario_reduction_assignment['cluster_label'].replace(scenario_reset_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the probability of each center as the sum of the probabilities of the scenarios that are assigned to that center.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_probability =  scenario_reduction_assignment.reset_index().groupby('cluster_label').count()\n",
    "scenario_probability = scenario_probability.rename(columns={'scenarios': 'n_scenarios'})\n",
    "scenario_probability.index.names = ['scenarios']\n",
    "scenario_probability['probability'] = scenario_probability['n_scenarios'] / N_SCENARIOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the reduced number of scenarios as well as their probabilities and can use these as input for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(PATH_SCENARIO_PROBABILITY), exist_ok=True)\n",
    "scenario_probability.to_pickle(PATH_SCENARIO_PROBABILITY)\n",
    "\n",
    "os.makedirs(os.path.dirname(PATH_SCENARIOS_REDUCED), exist_ok=True)\n",
    "selected_scenarios.to_pickle(PATH_SCENARIOS_REDUCED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "scenarios          8\n",
       "start_hex_ids     80\n",
       "end_hex_ids       80\n",
       "time               2\n",
       "vehicle_types      3\n",
       "demand           130\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "selected_scenarios.reset_index().nunique()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "921dce5431bc8dd5032dbf768a24c906ac17844f87e0f443ecd3f0f6d3f457c9"
  },
  "kernelspec": {
   "name": "python3810jvsc74a57bd0921dce5431bc8dd5032dbf768a24c906ac17844f87e0f443ecd3f0f6d3f457c9",
   "display_name": "Python 3.8.10 64-bit ('VR': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "98dda969eced25373352d8358f4a74bbe75f82753a7539c7817bd98c04161209"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}